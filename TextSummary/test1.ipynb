{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b4e559",
   "metadata": {},
   "source": [
    "## Пробуем суммаризировать тексты\n",
    "\n",
    "Используя модель bert, находим схожесть (cosine distance) между всеми предложениями текста.\n",
    "\n",
    "Строим граф узлами которого являются предложения текста, а длина связей определена найденным ранее значением схожести, чем более схожы предложения, тем ближе они между собой в графе.\n",
    "\n",
    "Далее алгоритмом Краскала находим минимальное оставное дерево в графе.\n",
    "\n",
    "Ограничением при построении дерева служит указанное минимальное количество узлов.\n",
    "\n",
    "Построение останавливается когда не останется деревьев с количеством узлов больше чем указано в ограничении.\n",
    "\n",
    "Минимальное оставное дерево - это суммаризация текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1d1a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000, -1.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([[1, 2, 3, 4],\n",
    "                        [1, 2, 3, 4]], dtype=float)\n",
    "preds = torch.tensor([[1, 2, 3, 4],\n",
    "                       [-1, -2, -3, -4]], dtype=float)\n",
    "\n",
    "torch.cosine_similarity(preds, target, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = \"cointegrated/rubert-tiny2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b7ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_version)\n",
    "model = AutoModel.from_pretrained(model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts  = [\n",
    "    'привет',\n",
    "    'всем',\n",
    "    'много',\n",
    "    'лет'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(\n",
    "    texts, # the texts to be tokenized\n",
    "    padding=True, # pad the texts to the maximum length (so that all outputs have the same length)\n",
    "    return_tensors='pt' # return the tensors (not lists)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = encodings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable gradient calculations\n",
    "with torch.no_grad():\n",
    "    # get the model embeddings\n",
    "    embeds = model(**encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = embeds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 312])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8782, 0.8561, 0.9206])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(embeds[0], embeds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 312])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MEANS = embeds.mean(dim=1)\n",
    "MEANS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8926)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(MEANS[0], MEANS[1], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_text_matrix(texts, model, device):\n",
    "\n",
    "    encodings = tokenizer(\n",
    "        texts, # the texts to be tokenized\n",
    "        padding=True, # pad the texts to the maximum length (so that all outputs have the same length)\n",
    "        return_tensors='pt' # return the tensors (not lists)\n",
    "    )\n",
    "\n",
    "    encodings = encodings.to(device)\n",
    "\n",
    "    # disable gradient calculations\n",
    "    with torch.no_grad():\n",
    "        # get the model embeddings\n",
    "        embeds = model(**encodings)\n",
    "\n",
    "    embeds = embeds[0]\n",
    "    means = embeds.mean(dim=1)\n",
    "\n",
    "    texts_len = len(texts)\n",
    "    matrix = np.eye(texts_len, dtype=float)\n",
    "\n",
    "    for i in range(0, texts_len):\n",
    "        for j in range(0, texts_len):\n",
    "            if i != j:\n",
    "                sim = torch.cosine_similarity(means[i], means[j], dim=0).item()\n",
    "                matrix[i, j] = sim\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_df(words, matrix):\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.columns = words\n",
    "    df.insert(0, '', words)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>всем</th>\n",
       "      <th>привет</th>\n",
       "      <th>df8</th>\n",
       "      <th>как</th>\n",
       "      <th>23</th>\n",
       "      <th>ваши</th>\n",
       "      <th>дела</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>всем</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.912513</td>\n",
       "      <td>0.532773</td>\n",
       "      <td>0.897429</td>\n",
       "      <td>0.804247</td>\n",
       "      <td>0.918380</td>\n",
       "      <td>0.896599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>привет</td>\n",
       "      <td>0.912513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.546540</td>\n",
       "      <td>0.907677</td>\n",
       "      <td>0.804162</td>\n",
       "      <td>0.923882</td>\n",
       "      <td>0.891256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>df8</td>\n",
       "      <td>0.532773</td>\n",
       "      <td>0.546540</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.564786</td>\n",
       "      <td>0.574964</td>\n",
       "      <td>0.562888</td>\n",
       "      <td>0.538052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>как</td>\n",
       "      <td>0.897429</td>\n",
       "      <td>0.907677</td>\n",
       "      <td>0.564786</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833596</td>\n",
       "      <td>0.921991</td>\n",
       "      <td>0.916361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0.804247</td>\n",
       "      <td>0.804162</td>\n",
       "      <td>0.574964</td>\n",
       "      <td>0.833596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818882</td>\n",
       "      <td>0.811977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ваши</td>\n",
       "      <td>0.918380</td>\n",
       "      <td>0.923882</td>\n",
       "      <td>0.562888</td>\n",
       "      <td>0.921991</td>\n",
       "      <td>0.818882</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>дела</td>\n",
       "      <td>0.896599</td>\n",
       "      <td>0.891256</td>\n",
       "      <td>0.538052</td>\n",
       "      <td>0.916361</td>\n",
       "      <td>0.811977</td>\n",
       "      <td>0.925869</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               всем    привет       df8       как        23      ваши  \\\n",
       "0    всем  1.000000  0.912513  0.532773  0.897429  0.804247  0.918380   \n",
       "1  привет  0.912513  1.000000  0.546540  0.907677  0.804162  0.923882   \n",
       "2     df8  0.532773  0.546540  1.000000  0.564786  0.574964  0.562888   \n",
       "3     как  0.897429  0.907677  0.564786  1.000000  0.833596  0.921991   \n",
       "4      23  0.804247  0.804162  0.574964  0.833596  1.000000  0.818882   \n",
       "5    ваши  0.918380  0.923882  0.562888  0.921991  0.818882  1.000000   \n",
       "6    дела  0.896599  0.891256  0.538052  0.916361  0.811977  0.925869   \n",
       "\n",
       "       дела  \n",
       "0  0.896599  \n",
       "1  0.891256  \n",
       "2  0.538052  \n",
       "3  0.916361  \n",
       "4  0.811977  \n",
       "5  0.925869  \n",
       "6  1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"всем привет df8 как 23 ваши дела\"\n",
    "words = text.split(' ')\n",
    "\n",
    "matrix = get_similarity_text_matrix(words, model, device)\n",
    "get_matrix_df(words, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
